## 리랭커 IR 벤치마크 (2025년 6월 9일 기준)
* 현실적인 응용에서 리랭커를 평가할 때는 인퍼런스 시간, 정확도(accuracy), 재현율(recall) 등 다양한 관점을 종합적으로 고려해야 합니다. 따라서 순위를 매기기보다는 모든 데이터를 투명하게 공개하여 편견 없는 비교를 제공합니다.

### AutoRAG, Korean, Cross-Encoder (Reranker)
* 대상 언어는 한국어이며, Task는 정보 검색(IR, Reranking)입니다.


| **Model Name**                             | **Params** | **Accuracy@1** | **Accuracy@3** | **Accuracy@5** | **Accuracy@10** | **F1@1** | **F1@3** | **F1@5** | **F1@10** | **Inference Time (s)** | **Avg Inference Time/query (s)** |
|--------------------------------------------|-----------|----------------|----------------|----------------|-----------------|----------|----------|----------|-----------|-----------------------|----------------------------------|
| dragonkue/bge-reranker-v2-m3-ko            | 568M      | 0.912          | 0.965          | 0.965          | 0.974           | 0.912    | 0.482    | 0.322    | 0.177     | 310.29               | 2.72                             |
| sigridjineth/ko-reranker-v1.1              | 306M      | 0.807          | 0.921          | 0.947          | 0.974           | 0.807    | 0.461    | 0.316    | 0.177     | 142.64               | 1.25                             |
| sigridjineth/ko-reranker-v1.2-preview      | 306M      | 0.877       | 0.947          | 0.965          | 0.974           | 0.877    | 0.474    | 0.322    | 0.177     | 219.37               | 1.92                             |
| Alibaba-NLP/gte-multilingual-reranker-base | 306M         | 0.728    | 0.921          | 0.947          | 0.974           | 0.728    | 0.461    | 0.316    | 0.177     | 252.35               | 2.21                             |
| upskyy/ko-reranker-8k                      | 568M      | 0.868          | 0.956          | 0.965          | 0.982           | 0.868    | 0.478    | 0.322    | 0.179     | 338.31               | 2.97                             |
| Dongjin-kr/ko-reranker                     | 560M      | 0.851          | 0.947          | 0.965          | 0.982           | 0.851    | 0.474    | 0.322    | 0.179     | 231.00               | 2.03                             |
| jinaai/jina-reranker-v2-base-multilingual  | 278M      | 0.807          | 0.930          | 0.939          | 0.947           | 0.807    | 0.465    | 0.313    | 0.172     | 216.56               | 1.90                             |
| BAAI/bge-reranker-v2-m3                    | 568M      | 0.877          | 0.956          | 0.965          | 0.991           | 0.877    | 0.478    | 0.322    | 0.180     | 313.88               | 2.75                             |
| BAAI/bge-reranker-v2-gemma                 | 2B        | 0.772          | 0.877          | 0.912          | 0.947           | 0.772    | 0.439    | 0.304    | 0.172     | 1335.19              | 11.71                            |
| BAAI/bge-reranker-v2.5-gemma2-lightweight  | 9B        | 0.816          | 0.912          | 0.930          | 0.947           | 0.816    | 0.456    | 0.310    | 0.172     | 5052.88              | 44.32                            |
| mixedbread-ai/mxbai-rerank-large-v2        | 1.5B      | 0.885          | 0.991          | 1.0            | 1.0             | 0.885    | 0.495    | 0.333    | 0.181     | 2373.90              | 20.82                            |
| michaelfeil/mxbai-rerank-base-v2-seq       | 0.5B      | 0.8421 | 0.9737         | 1.0000 | 1.0000 | 0.8421 | 0.4868 | 0.3333 | 0.1818 | 1055.39 | 9.2578 |
| Qwen/Qwen3-Reranker-0.6B | 0.6B      | 0.833          | 0.982          | 0.991          | 1.000           | 0.833    | 0.491    | 0.330    | 0.182     | 4179.76              | 36.66                            |